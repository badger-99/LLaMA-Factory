version: '3.8'

services:
  llama-factory:
    build:
      dockerfile: Dockerfile
      context: .
    container_name: llama_factory
    volumes:
      - ./hf_cache:/root/.cache/huggingface/
      - ./data:/app/data
      - ./output:/app/output
      - ./models:/app/models
      - ./configs:/app/configs
    ports:
      - "7860:7860"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: '3'
              capabilities: [gpu]
    restart: unless-stopped
